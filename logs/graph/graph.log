[INFO|config|L150] 2024-09-29 23:02:34: Graph Agents : {'query_classificator': Agent(agent_name='query_classificator', model='GROQ', get_model=<function get_groq at 0x0000023591127560>, temperature=0, prompt=ChatPromptTemplate(input_variables=['labels', 'text'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['labels'], input_types={}, partial_variables={}, template="You are an assistant specialized in categorizing a text from the Spanish Boletín Oficial del Estado (BOE).\n\n            Your task is to classify the provided text using the specified list of labels. The possible labels are: {labels}\n\n            If the text does not belong to a label, classify it as 'Otra'. Provide the label with no preamble or explanation"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='{text}'), additional_kwargs={})]), parser=<class 'langchain_core.output_parsers.string.StrOutputParser'>), 'docs_grader': Agent(agent_name='docs_grader', model='GROQ', get_model=<function get_groq at 0x0000023591127560>, temperature=0, prompt=ChatPromptTemplate(input_variables=['document', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['document'], input_types={}, partial_variables={}, template="You are an AI model designed grade the relevance of a retrieved document to a user question.\n\n                If the document contains keywords related to the user question, grade it as relevant.\n\n                Give a binary score of 'yes' or 'no' to indicate whether the document is relevant to the question. \n\n                Provide the binary score with no explanation.\n\n                Here is the retrieved document: {document} "), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})]), parser=<class 'langchain_core.output_parsers.string.StrOutputParser'>), 'query_processor': Agent(agent_name='query_processor', model='GROQ', get_model=<function get_groq at 0x0000023591127560>, temperature=0, prompt=ChatPromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a question re-writer that converts an input question to a better version that is optimized for vectorstore retrieval. \n\n                    Look at the input and try to reason about the underlying semantic intent / meaning.\n\n                    Provide the reprocessed question in spanish with no preamble or explanation'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})]), parser=<class 'langchain_core.output_parsers.string.StrOutputParser'>), 'generator': Agent(agent_name='generator', model='GROQ', get_model=<function get_groq at 0x0000023591127560>, temperature=0, prompt=ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template="You are an assistant for question-answering tasks.\n\n                    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say 'I you don't know'.\n\n                    Use three sentences maximum and keep the answer concise.\n\n                    Context:\n{context}\n"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})]), parser=<class 'langchain_core.output_parsers.string.StrOutputParser'>), 'hallucination_grader': Agent(agent_name='hallucination_grader', model='GROQ', get_model=<function get_groq at 0x0000023591127560>, temperature=0, prompt=ChatPromptTemplate(input_variables=['documents', 'generation'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['documents'], input_types={}, partial_variables={}, template="You are a grader assessing whether an answer is grounded in / supported by a set of facts. \n \n                Give a binary score 'yes' or 'no' score to indicate\n\n                whether the answer is grounded in or supported by a set of facts. Provide the binary score with no preamble or explanation.\n\n                Here are the facts:\n{documents}\n"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['generation'], input_types={}, partial_variables={}, template='{generation}'), additional_kwargs={})]), parser=<class 'langchain_core.output_parsers.string.StrOutputParser'>), 'answer_grader': Agent(agent_name='answer_grader', model='GROQ', get_model=<function get_groq at 0x0000023591127560>, temperature=0, prompt=ChatPromptTemplate(input_variables=['generation', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template="You are a grader assessing whether an  answer is useful to resolve a question.\n\n                Give a binary score 'yes' or 'no' to indicate whether the answer is useful to resolve a question.\n\n                Provide the binary score with no preamble or explanation.\n                Here is the question:\n{question}\n"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['generation'], input_types={}, partial_variables={}, template='{generation}'), additional_kwargs={})]), parser=<class 'langchain_core.output_parsers.string.StrOutputParser'>)}
[INFO|config|L229] 2024-09-29 23:02:34: Connecting to an existing index of PineCone DB cient
[INFO|discover_namespace_packages|L12] 2024-09-29 23:02:34: Discovering subpackages in _NamespacePath(['C:\\Users\\Jorge\\Desktop\\Boe ChatBot\\backend\\env\\Lib\\site-packages\\pinecone_plugins'])
[INFO|discover_plugins|L9] 2024-09-29 23:02:34: Looking for plugins in pinecone_plugins.assistant
[INFO|discover_plugins|L9] 2024-09-29 23:02:34: Looking for plugins in pinecone_plugins.inference
[INFO|installation|L10] 2024-09-29 23:02:34: Installing plugin assistant into Pinecone
[INFO|installation|L10] 2024-09-29 23:02:34: Installing plugin inference into Pinecone
[INFO|config|L236] 2024-09-29 23:02:36: existing_indexes : ['boe']
[INFO|SentenceTransformer|L196] 2024-09-29 23:02:36: Use pytorch device_name: cpu
[INFO|SentenceTransformer|L204] 2024-09-29 23:02:36: Load pretrained SentenceTransformer: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2
[INFO|main|L67] 2024-09-29 23:02:39: config_graph : ConfigGraph(config_path='C:\\Users\\Jorge\\Desktop\\Boe ChatBot\\backend\\src\\..\\config/graph\\graph.json', graph=None, compile_graph=None)
[INFO|main|L69] 2024-09-29 23:02:39: Creating graph and compiling workflow...
[INFO|main|L81] 2024-09-29 23:02:39: Graph and workflow created
[INFO|main|L65] 2024-09-29 23:05:11: Getting Graph configuration from CONFIG_PATH='C:\\Users\\Jorge\\Desktop\\Boe ChatBot\\backend\\src\\..\\config/graph\\graph.json'
[INFO|config|L93] 2024-09-29 23:05:11: Definida configuracion mediante archivo JSON en C:\Users\Jorge\Desktop\Boe ChatBot\backend\src\..\config/graph\graph.json
[INFO|config|L224] 2024-09-29 23:05:11: Agent query_classificator -> Parser <class 'langchain_core.output_parsers.string.StrOutputParser'>
[INFO|config|L224] 2024-09-29 23:05:11: Agent docs_grader -> Parser <class 'langchain_core.output_parsers.string.StrOutputParser'>
[INFO|config|L224] 2024-09-29 23:05:11: Agent query_processor -> Parser <class 'langchain_core.output_parsers.string.StrOutputParser'>
[INFO|config|L224] 2024-09-29 23:05:11: Agent generator -> Parser <class 'langchain_core.output_parsers.string.StrOutputParser'>
[INFO|config|L224] 2024-09-29 23:05:11: Agent hallucination_grader -> Parser <class 'langchain_core.output_parsers.string.StrOutputParser'>
[INFO|config|L224] 2024-09-29 23:05:11: Agent answer_grader -> Parser <class 'langchain_core.output_parsers.string.StrOutputParser'>
[INFO|config|L150] 2024-09-29 23:05:11: Graph Agents : {'query_classificator': Agent(agent_name='query_classificator', model='GROQ', get_model=<function get_groq at 0x000001668DF67740>, temperature=0, prompt=ChatPromptTemplate(input_variables=['labels', 'text'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['labels'], input_types={}, partial_variables={}, template="You are an assistant specialized in categorizing a text from the Spanish Boletín Oficial del Estado (BOE).\n\n            Your task is to classify the provided text using the specified list of labels. The possible labels are: {labels}\n\n            If the text does not belong to a label, classify it as 'Otra'. Provide the label with no preamble or explanation"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='{text}'), additional_kwargs={})]), parser=<class 'langchain_core.output_parsers.string.StrOutputParser'>), 'docs_grader': Agent(agent_name='docs_grader', model='GROQ', get_model=<function get_groq at 0x000001668DF67740>, temperature=0, prompt=ChatPromptTemplate(input_variables=['document', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['document'], input_types={}, partial_variables={}, template="You are an AI model designed grade the relevance of a retrieved document to a user question.\n\n                If the document contains keywords related to the user question, grade it as relevant.\n\n                Give a binary score of 'yes' or 'no' to indicate whether the document is relevant to the question. \n\n                Provide the binary score with no explanation.\n\n                Here is the retrieved document: {document} "), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})]), parser=<class 'langchain_core.output_parsers.string.StrOutputParser'>), 'query_processor': Agent(agent_name='query_processor', model='GROQ', get_model=<function get_groq at 0x000001668DF67740>, temperature=0, prompt=ChatPromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a question re-writer that converts an input question to a better version that is optimized for vectorstore retrieval. \n\n                    Look at the input and try to reason about the underlying semantic intent / meaning.\n\n                    Provide the reprocessed question in spanish with no preamble or explanation'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})]), parser=<class 'langchain_core.output_parsers.string.StrOutputParser'>), 'generator': Agent(agent_name='generator', model='GROQ', get_model=<function get_groq at 0x000001668DF67740>, temperature=0, prompt=ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template="You are an assistant for question-answering tasks.\n\n                    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say 'I you don't know'.\n\n                    Use three sentences maximum and keep the answer concise.\n\n                    Context:\n{context}\n"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})]), parser=<class 'langchain_core.output_parsers.string.StrOutputParser'>), 'hallucination_grader': Agent(agent_name='hallucination_grader', model='GROQ', get_model=<function get_groq at 0x000001668DF67740>, temperature=0, prompt=ChatPromptTemplate(input_variables=['documents', 'generation'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['documents'], input_types={}, partial_variables={}, template="You are a grader assessing whether an answer is grounded in / supported by a set of facts. \n \n                Give a binary score 'yes' or 'no' score to indicate\n\n                whether the answer is grounded in or supported by a set of facts. Provide the binary score with no preamble or explanation.\n\n                Here are the facts:\n{documents}\n"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['generation'], input_types={}, partial_variables={}, template='{generation}'), additional_kwargs={})]), parser=<class 'langchain_core.output_parsers.string.StrOutputParser'>), 'answer_grader': Agent(agent_name='answer_grader', model='GROQ', get_model=<function get_groq at 0x000001668DF67740>, temperature=0, prompt=ChatPromptTemplate(input_variables=['generation', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template="You are a grader assessing whether an  answer is useful to resolve a question.\n\n                Give a binary score 'yes' or 'no' to indicate whether the answer is useful to resolve a question.\n\n                Provide the binary score with no preamble or explanation.\n                Here is the question:\n{question}\n"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['generation'], input_types={}, partial_variables={}, template='{generation}'), additional_kwargs={})]), parser=<class 'langchain_core.output_parsers.string.StrOutputParser'>)}
[INFO|config|L229] 2024-09-29 23:05:11: Connecting to an existing index of PineCone DB cient
[INFO|discover_namespace_packages|L12] 2024-09-29 23:05:11: Discovering subpackages in _NamespacePath(['C:\\Users\\Jorge\\Desktop\\Boe ChatBot\\backend\\env\\Lib\\site-packages\\pinecone_plugins'])
[INFO|discover_plugins|L9] 2024-09-29 23:05:11: Looking for plugins in pinecone_plugins.assistant
[INFO|discover_plugins|L9] 2024-09-29 23:05:11: Looking for plugins in pinecone_plugins.inference
[INFO|installation|L10] 2024-09-29 23:05:11: Installing plugin assistant into Pinecone
[INFO|installation|L10] 2024-09-29 23:05:11: Installing plugin inference into Pinecone
[INFO|config|L236] 2024-09-29 23:05:11: existing_indexes : ['boe']
[INFO|SentenceTransformer|L196] 2024-09-29 23:05:12: Use pytorch device_name: cpu
[INFO|SentenceTransformer|L204] 2024-09-29 23:05:12: Load pretrained SentenceTransformer: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2
[INFO|main|L67] 2024-09-29 23:05:14: config_graph : ConfigGraph(config_path='C:\\Users\\Jorge\\Desktop\\Boe ChatBot\\backend\\src\\..\\config/graph\\graph.json', graph=None, compile_graph=None)
[INFO|main|L69] 2024-09-29 23:05:14: Creating graph and compiling workflow...
[INFO|main|L81] 2024-09-29 23:05:14: Graph and workflow created
[INFO|main|L65] 2024-09-29 23:06:35: Getting Graph configuration from CONFIG_PATH='C:\\Users\\Jorge\\Desktop\\Boe ChatBot\\backend\\src\\..\\config/graph\\graph.json'
[INFO|config|L93] 2024-09-29 23:06:35: Definida configuracion mediante archivo JSON en C:\Users\Jorge\Desktop\Boe ChatBot\backend\src\..\config/graph\graph.json
[INFO|config|L224] 2024-09-29 23:06:35: Agent query_classificator -> Parser <class 'langchain_core.output_parsers.string.StrOutputParser'>
[INFO|config|L224] 2024-09-29 23:06:35: Agent docs_grader -> Parser <class 'langchain_core.output_parsers.string.StrOutputParser'>
[INFO|config|L224] 2024-09-29 23:06:35: Agent query_processor -> Parser <class 'langchain_core.output_parsers.string.StrOutputParser'>
[INFO|config|L224] 2024-09-29 23:06:35: Agent generator -> Parser <class 'langchain_core.output_parsers.string.StrOutputParser'>
[INFO|config|L224] 2024-09-29 23:06:35: Agent hallucination_grader -> Parser <class 'langchain_core.output_parsers.string.StrOutputParser'>
[INFO|config|L224] 2024-09-29 23:06:35: Agent answer_grader -> Parser <class 'langchain_core.output_parsers.string.StrOutputParser'>
[INFO|config|L150] 2024-09-29 23:06:35: Graph Agents : {'query_classificator': Agent(agent_name='query_classificator', model='GROQ', get_model=<function get_groq at 0x000002029D7C7740>, temperature=0, prompt=ChatPromptTemplate(input_variables=['labels', 'text'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['labels'], input_types={}, partial_variables={}, template="You are an assistant specialized in categorizing a text from the Spanish Boletín Oficial del Estado (BOE).\n\n            Your task is to classify the provided text using the specified list of labels. The possible labels are: {labels}\n\n            If the text does not belong to a label, classify it as 'Otra'. Provide the label with no preamble or explanation"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='{text}'), additional_kwargs={})]), parser=<class 'langchain_core.output_parsers.string.StrOutputParser'>), 'docs_grader': Agent(agent_name='docs_grader', model='GROQ', get_model=<function get_groq at 0x000002029D7C7740>, temperature=0, prompt=ChatPromptTemplate(input_variables=['document', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['document'], input_types={}, partial_variables={}, template="You are an AI model designed grade the relevance of a retrieved document to a user question.\n\n                If the document contains keywords related to the user question, grade it as relevant.\n\n                Give a binary score of 'yes' or 'no' to indicate whether the document is relevant to the question. \n\n                Provide the binary score with no explanation.\n\n                Here is the retrieved document: {document} "), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})]), parser=<class 'langchain_core.output_parsers.string.StrOutputParser'>), 'query_processor': Agent(agent_name='query_processor', model='GROQ', get_model=<function get_groq at 0x000002029D7C7740>, temperature=0, prompt=ChatPromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a question re-writer that converts an input question to a better version that is optimized for vectorstore retrieval. \n\n                    Look at the input and try to reason about the underlying semantic intent / meaning.\n\n                    Provide the reprocessed question in spanish with no preamble or explanation'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})]), parser=<class 'langchain_core.output_parsers.string.StrOutputParser'>), 'generator': Agent(agent_name='generator', model='GROQ', get_model=<function get_groq at 0x000002029D7C7740>, temperature=0, prompt=ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template="You are an assistant for question-answering tasks.\n\n                    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say 'I you don't know'.\n\n                    Use three sentences maximum and keep the answer concise.\n\n                    Context:\n{context}\n"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})]), parser=<class 'langchain_core.output_parsers.string.StrOutputParser'>), 'hallucination_grader': Agent(agent_name='hallucination_grader', model='GROQ', get_model=<function get_groq at 0x000002029D7C7740>, temperature=0, prompt=ChatPromptTemplate(input_variables=['documents', 'generation'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['documents'], input_types={}, partial_variables={}, template="You are a grader assessing whether an answer is grounded in / supported by a set of facts. \n \n                Give a binary score 'yes' or 'no' score to indicate\n\n                whether the answer is grounded in or supported by a set of facts. Provide the binary score with no preamble or explanation.\n\n                Here are the facts:\n{documents}\n"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['generation'], input_types={}, partial_variables={}, template='{generation}'), additional_kwargs={})]), parser=<class 'langchain_core.output_parsers.string.StrOutputParser'>), 'answer_grader': Agent(agent_name='answer_grader', model='GROQ', get_model=<function get_groq at 0x000002029D7C7740>, temperature=0, prompt=ChatPromptTemplate(input_variables=['generation', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template="You are a grader assessing whether an  answer is useful to resolve a question.\n\n                Give a binary score 'yes' or 'no' to indicate whether the answer is useful to resolve a question.\n\n                Provide the binary score with no preamble or explanation.\n                Here is the question:\n{question}\n"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['generation'], input_types={}, partial_variables={}, template='{generation}'), additional_kwargs={})]), parser=<class 'langchain_core.output_parsers.string.StrOutputParser'>)}
[INFO|config|L229] 2024-09-29 23:06:35: Connecting to an existing index of PineCone DB cient
[INFO|discover_namespace_packages|L12] 2024-09-29 23:06:35: Discovering subpackages in _NamespacePath(['C:\\Users\\Jorge\\Desktop\\Boe ChatBot\\backend\\env\\Lib\\site-packages\\pinecone_plugins'])
[INFO|discover_plugins|L9] 2024-09-29 23:06:35: Looking for plugins in pinecone_plugins.assistant
[INFO|discover_plugins|L9] 2024-09-29 23:06:35: Looking for plugins in pinecone_plugins.inference
[INFO|installation|L10] 2024-09-29 23:06:35: Installing plugin assistant into Pinecone
[INFO|installation|L10] 2024-09-29 23:06:35: Installing plugin inference into Pinecone
[INFO|config|L236] 2024-09-29 23:06:35: existing_indexes : ['boe']
[INFO|SentenceTransformer|L196] 2024-09-29 23:06:36: Use pytorch device_name: cpu
[INFO|SentenceTransformer|L204] 2024-09-29 23:06:36: Load pretrained SentenceTransformer: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2
[INFO|main|L67] 2024-09-29 23:06:38: config_graph : ConfigGraph(config_path='C:\\Users\\Jorge\\Desktop\\Boe ChatBot\\backend\\src\\..\\config/graph\\graph.json', graph=None, compile_graph=None)
[INFO|main|L69] 2024-09-29 23:06:38: Creating graph and compiling workflow...
[INFO|main|L81] 2024-09-29 23:06:38: Graph and workflow created
[INFO|main|L65] 2024-09-29 23:08:22: Getting Graph configuration from CONFIG_PATH='C:\\Users\\Jorge\\Desktop\\Boe ChatBot\\backend\\src\\..\\config/graph\\graph.json'
[INFO|config|L93] 2024-09-29 23:08:22: Definida configuracion mediante archivo JSON en C:\Users\Jorge\Desktop\Boe ChatBot\backend\src\..\config/graph\graph.json
[INFO|config|L224] 2024-09-29 23:08:22: Agent query_classificator -> Parser <class 'langchain_core.output_parsers.string.StrOutputParser'>
[INFO|config|L224] 2024-09-29 23:08:22: Agent docs_grader -> Parser <class 'langchain_core.output_parsers.string.StrOutputParser'>
[INFO|config|L224] 2024-09-29 23:08:22: Agent query_processor -> Parser <class 'langchain_core.output_parsers.string.StrOutputParser'>
[INFO|config|L224] 2024-09-29 23:08:22: Agent generator -> Parser <class 'langchain_core.output_parsers.string.StrOutputParser'>
[INFO|config|L224] 2024-09-29 23:08:22: Agent hallucination_grader -> Parser <class 'langchain_core.output_parsers.string.StrOutputParser'>
[INFO|config|L224] 2024-09-29 23:08:22: Agent answer_grader -> Parser <class 'langchain_core.output_parsers.string.StrOutputParser'>
[INFO|config|L150] 2024-09-29 23:08:22: Graph Agents : {'query_classificator': Agent(agent_name='query_classificator', model='GROQ', get_model=<function get_groq at 0x0000022C24977740>, temperature=0, prompt=ChatPromptTemplate(input_variables=['labels', 'text'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['labels'], input_types={}, partial_variables={}, template="You are an assistant specialized in categorizing a text from the Spanish Boletín Oficial del Estado (BOE).\n\n            Your task is to classify the provided text using the specified list of labels. The possible labels are: {labels}\n\n            If the text does not belong to a label, classify it as 'Otra'. Provide the label with no preamble or explanation"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='{text}'), additional_kwargs={})]), parser=<class 'langchain_core.output_parsers.string.StrOutputParser'>), 'docs_grader': Agent(agent_name='docs_grader', model='GROQ', get_model=<function get_groq at 0x0000022C24977740>, temperature=0, prompt=ChatPromptTemplate(input_variables=['document', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['document'], input_types={}, partial_variables={}, template="You are an AI model designed grade the relevance of a retrieved document to a user question.\n\n                If the document contains keywords related to the user question, grade it as relevant.\n\n                Give a binary score of 'yes' or 'no' to indicate whether the document is relevant to the question. \n\n                Provide the binary score with no explanation.\n\n                Here is the retrieved document: {document} "), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})]), parser=<class 'langchain_core.output_parsers.string.StrOutputParser'>), 'query_processor': Agent(agent_name='query_processor', model='GROQ', get_model=<function get_groq at 0x0000022C24977740>, temperature=0, prompt=ChatPromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a question re-writer that converts an input question to a better version that is optimized for vectorstore retrieval. \n\n                    Look at the input and try to reason about the underlying semantic intent / meaning.\n\n                    Provide the reprocessed question in spanish with no preamble or explanation'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})]), parser=<class 'langchain_core.output_parsers.string.StrOutputParser'>), 'generator': Agent(agent_name='generator', model='GROQ', get_model=<function get_groq at 0x0000022C24977740>, temperature=0, prompt=ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template="You are an assistant for question-answering tasks.\n\n                    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say 'I you don't know'.\n\n                    Use three sentences maximum and keep the answer concise.\n\n                    Context:\n{context}\n"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})]), parser=<class 'langchain_core.output_parsers.string.StrOutputParser'>), 'hallucination_grader': Agent(agent_name='hallucination_grader', model='GROQ', get_model=<function get_groq at 0x0000022C24977740>, temperature=0, prompt=ChatPromptTemplate(input_variables=['documents', 'generation'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['documents'], input_types={}, partial_variables={}, template="You are a grader assessing whether an answer is grounded in / supported by a set of facts. \n \n                Give a binary score 'yes' or 'no' score to indicate\n\n                whether the answer is grounded in or supported by a set of facts. Provide the binary score with no preamble or explanation.\n\n                Here are the facts:\n{documents}\n"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['generation'], input_types={}, partial_variables={}, template='{generation}'), additional_kwargs={})]), parser=<class 'langchain_core.output_parsers.string.StrOutputParser'>), 'answer_grader': Agent(agent_name='answer_grader', model='GROQ', get_model=<function get_groq at 0x0000022C24977740>, temperature=0, prompt=ChatPromptTemplate(input_variables=['generation', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template="You are a grader assessing whether an  answer is useful to resolve a question.\n\n                Give a binary score 'yes' or 'no' to indicate whether the answer is useful to resolve a question.\n\n                Provide the binary score with no preamble or explanation.\n                Here is the question:\n{question}\n"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['generation'], input_types={}, partial_variables={}, template='{generation}'), additional_kwargs={})]), parser=<class 'langchain_core.output_parsers.string.StrOutputParser'>)}
[INFO|config|L229] 2024-09-29 23:08:22: Connecting to an existing index of PineCone DB cient
[INFO|discover_namespace_packages|L12] 2024-09-29 23:08:22: Discovering subpackages in _NamespacePath(['C:\\Users\\Jorge\\Desktop\\Boe ChatBot\\backend\\env\\Lib\\site-packages\\pinecone_plugins'])
[INFO|discover_plugins|L9] 2024-09-29 23:08:22: Looking for plugins in pinecone_plugins.assistant
[INFO|discover_plugins|L9] 2024-09-29 23:08:22: Looking for plugins in pinecone_plugins.inference
[INFO|installation|L10] 2024-09-29 23:08:22: Installing plugin assistant into Pinecone
[INFO|installation|L10] 2024-09-29 23:08:22: Installing plugin inference into Pinecone
[INFO|config|L236] 2024-09-29 23:08:22: existing_indexes : ['boe']
[INFO|SentenceTransformer|L196] 2024-09-29 23:08:23: Use pytorch device_name: cpu
[INFO|SentenceTransformer|L204] 2024-09-29 23:08:23: Load pretrained SentenceTransformer: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2
[INFO|main|L67] 2024-09-29 23:08:25: config_graph : ConfigGraph(config_path='C:\\Users\\Jorge\\Desktop\\Boe ChatBot\\backend\\src\\..\\config/graph\\graph.json', graph=None, compile_graph=None)
[INFO|main|L69] 2024-09-29 23:08:25: Creating graph and compiling workflow...
[INFO|main|L81] 2024-09-29 23:08:25: Graph and workflow created
[INFO|main|L65] 2024-09-29 23:10:59: Getting Graph configuration from CONFIG_PATH='C:\\Users\\Jorge\\Desktop\\Boe ChatBot\\backend\\src\\..\\config/graph\\graph.json'
[INFO|config|L93] 2024-09-29 23:10:59: Definida configuracion mediante archivo JSON en C:\Users\Jorge\Desktop\Boe ChatBot\backend\src\..\config/graph\graph.json
[INFO|config|L224] 2024-09-29 23:10:59: Agent query_classificator -> Parser <class 'langchain_core.output_parsers.string.StrOutputParser'>
[INFO|config|L224] 2024-09-29 23:10:59: Agent docs_grader -> Parser <class 'langchain_core.output_parsers.string.StrOutputParser'>
[INFO|config|L224] 2024-09-29 23:10:59: Agent query_processor -> Parser <class 'langchain_core.output_parsers.string.StrOutputParser'>
[INFO|config|L224] 2024-09-29 23:10:59: Agent generator -> Parser <class 'langchain_core.output_parsers.string.StrOutputParser'>
[INFO|config|L224] 2024-09-29 23:10:59: Agent hallucination_grader -> Parser <class 'langchain_core.output_parsers.string.StrOutputParser'>
[INFO|config|L224] 2024-09-29 23:10:59: Agent answer_grader -> Parser <class 'langchain_core.output_parsers.string.StrOutputParser'>
[INFO|config|L150] 2024-09-29 23:10:59: Graph Agents : {'query_classificator': Agent(agent_name='query_classificator', model='GROQ', get_model=<function get_groq at 0x000002A139E277E0>, temperature=0, prompt=ChatPromptTemplate(input_variables=['labels', 'text'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['labels'], input_types={}, partial_variables={}, template="You are an assistant specialized in categorizing a text from the Spanish Boletín Oficial del Estado (BOE).\n\n            Your task is to classify the provided text using the specified list of labels. The possible labels are: {labels}\n\n            If the text does not belong to a label, classify it as 'Otra'. Provide the label with no preamble or explanation"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='{text}'), additional_kwargs={})]), parser=<class 'langchain_core.output_parsers.string.StrOutputParser'>), 'docs_grader': Agent(agent_name='docs_grader', model='GROQ', get_model=<function get_groq at 0x000002A139E277E0>, temperature=0, prompt=ChatPromptTemplate(input_variables=['document', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['document'], input_types={}, partial_variables={}, template="You are an AI model designed grade the relevance of a retrieved document to a user question.\n\n                If the document contains keywords related to the user question, grade it as relevant.\n\n                Give a binary score of 'yes' or 'no' to indicate whether the document is relevant to the question. \n\n                Provide the binary score with no explanation.\n\n                Here is the retrieved document: {document} "), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})]), parser=<class 'langchain_core.output_parsers.string.StrOutputParser'>), 'query_processor': Agent(agent_name='query_processor', model='GROQ', get_model=<function get_groq at 0x000002A139E277E0>, temperature=0, prompt=ChatPromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a question re-writer that converts an input question to a better version that is optimized for vectorstore retrieval. \n\n                    Look at the input and try to reason about the underlying semantic intent / meaning.\n\n                    Provide the reprocessed question in spanish with no preamble or explanation'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})]), parser=<class 'langchain_core.output_parsers.string.StrOutputParser'>), 'generator': Agent(agent_name='generator', model='GROQ', get_model=<function get_groq at 0x000002A139E277E0>, temperature=0, prompt=ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template="You are an assistant for question-answering tasks.\n\n                    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say 'I you don't know'.\n\n                    Use three sentences maximum and keep the answer concise.\n\n                    Context:\n{context}\n"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})]), parser=<class 'langchain_core.output_parsers.string.StrOutputParser'>), 'hallucination_grader': Agent(agent_name='hallucination_grader', model='GROQ', get_model=<function get_groq at 0x000002A139E277E0>, temperature=0, prompt=ChatPromptTemplate(input_variables=['documents', 'generation'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['documents'], input_types={}, partial_variables={}, template="You are a grader assessing whether an answer is grounded in / supported by a set of facts. \n \n                Give a binary score 'yes' or 'no' score to indicate\n\n                whether the answer is grounded in or supported by a set of facts. Provide the binary score with no preamble or explanation.\n\n                Here are the facts:\n{documents}\n"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['generation'], input_types={}, partial_variables={}, template='{generation}'), additional_kwargs={})]), parser=<class 'langchain_core.output_parsers.string.StrOutputParser'>), 'answer_grader': Agent(agent_name='answer_grader', model='GROQ', get_model=<function get_groq at 0x000002A139E277E0>, temperature=0, prompt=ChatPromptTemplate(input_variables=['generation', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template="You are a grader assessing whether an  answer is useful to resolve a question.\n\n                Give a binary score 'yes' or 'no' to indicate whether the answer is useful to resolve a question.\n\n                Provide the binary score with no preamble or explanation.\n                Here is the question:\n{question}\n"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['generation'], input_types={}, partial_variables={}, template='{generation}'), additional_kwargs={})]), parser=<class 'langchain_core.output_parsers.string.StrOutputParser'>)}
[INFO|config|L229] 2024-09-29 23:10:59: Connecting to an existing index of PineCone DB cient
[INFO|discover_namespace_packages|L12] 2024-09-29 23:10:59: Discovering subpackages in _NamespacePath(['C:\\Users\\Jorge\\Desktop\\Boe ChatBot\\backend\\env\\Lib\\site-packages\\pinecone_plugins'])
[INFO|discover_plugins|L9] 2024-09-29 23:10:59: Looking for plugins in pinecone_plugins.assistant
[INFO|discover_plugins|L9] 2024-09-29 23:10:59: Looking for plugins in pinecone_plugins.inference
[INFO|installation|L10] 2024-09-29 23:10:59: Installing plugin assistant into Pinecone
[INFO|installation|L10] 2024-09-29 23:10:59: Installing plugin inference into Pinecone
[INFO|config|L236] 2024-09-29 23:10:59: existing_indexes : ['boe']
[INFO|SentenceTransformer|L196] 2024-09-29 23:10:59: Use pytorch device_name: cpu
[INFO|SentenceTransformer|L204] 2024-09-29 23:10:59: Load pretrained SentenceTransformer: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2
[INFO|main|L67] 2024-09-29 23:11:02: config_graph : ConfigGraph(config_path='C:\\Users\\Jorge\\Desktop\\Boe ChatBot\\backend\\src\\..\\config/graph\\graph.json', graph=None, compile_graph=None)
[INFO|main|L69] 2024-09-29 23:11:02: Creating graph and compiling workflow...
[INFO|main|L81] 2024-09-29 23:11:02: Graph and workflow created
[INFO|main|L65] 2024-09-29 23:11:32: Getting Graph configuration from CONFIG_PATH='C:\\Users\\Jorge\\Desktop\\Boe ChatBot\\backend\\src\\..\\config/graph\\graph.json'
[INFO|config|L93] 2024-09-29 23:11:32: Definida configuracion mediante archivo JSON en C:\Users\Jorge\Desktop\Boe ChatBot\backend\src\..\config/graph\graph.json
[INFO|config|L224] 2024-09-29 23:11:32: Agent query_classificator -> Parser <class 'langchain_core.output_parsers.string.StrOutputParser'>
[INFO|config|L224] 2024-09-29 23:11:32: Agent docs_grader -> Parser <class 'langchain_core.output_parsers.string.StrOutputParser'>
[INFO|config|L224] 2024-09-29 23:11:32: Agent query_processor -> Parser <class 'langchain_core.output_parsers.string.StrOutputParser'>
[INFO|config|L224] 2024-09-29 23:11:32: Agent generator -> Parser <class 'langchain_core.output_parsers.string.StrOutputParser'>
[INFO|config|L224] 2024-09-29 23:11:32: Agent hallucination_grader -> Parser <class 'langchain_core.output_parsers.string.StrOutputParser'>
[INFO|config|L224] 2024-09-29 23:11:32: Agent answer_grader -> Parser <class 'langchain_core.output_parsers.string.StrOutputParser'>
[INFO|config|L150] 2024-09-29 23:11:32: Graph Agents : {'query_classificator': Agent(agent_name='query_classificator', model='GROQ', get_model=<function get_groq at 0x000001EBACAD77E0>, temperature=0, prompt=ChatPromptTemplate(input_variables=['labels', 'text'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['labels'], input_types={}, partial_variables={}, template="You are an assistant specialized in categorizing a text from the Spanish Boletín Oficial del Estado (BOE).\n\n            Your task is to classify the provided text using the specified list of labels. The possible labels are: {labels}\n\n            If the text does not belong to a label, classify it as 'Otra'. Provide the label with no preamble or explanation"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='{text}'), additional_kwargs={})]), parser=<class 'langchain_core.output_parsers.string.StrOutputParser'>), 'docs_grader': Agent(agent_name='docs_grader', model='GROQ', get_model=<function get_groq at 0x000001EBACAD77E0>, temperature=0, prompt=ChatPromptTemplate(input_variables=['document', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['document'], input_types={}, partial_variables={}, template="You are an AI model designed grade the relevance of a retrieved document to a user question.\n\n                If the document contains keywords related to the user question, grade it as relevant.\n\n                Give a binary score of 'yes' or 'no' to indicate whether the document is relevant to the question. \n\n                Provide the binary score with no explanation.\n\n                Here is the retrieved document: {document} "), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})]), parser=<class 'langchain_core.output_parsers.string.StrOutputParser'>), 'query_processor': Agent(agent_name='query_processor', model='GROQ', get_model=<function get_groq at 0x000001EBACAD77E0>, temperature=0, prompt=ChatPromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a question re-writer that converts an input question to a better version that is optimized for vectorstore retrieval. \n\n                    Look at the input and try to reason about the underlying semantic intent / meaning.\n\n                    Provide the reprocessed question in spanish with no preamble or explanation'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})]), parser=<class 'langchain_core.output_parsers.string.StrOutputParser'>), 'generator': Agent(agent_name='generator', model='GROQ', get_model=<function get_groq at 0x000001EBACAD77E0>, temperature=0, prompt=ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template="You are an assistant for question-answering tasks.\n\n                    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say 'I you don't know'.\n\n                    Use three sentences maximum and keep the answer concise.\n\n                    Context:\n{context}\n"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})]), parser=<class 'langchain_core.output_parsers.string.StrOutputParser'>), 'hallucination_grader': Agent(agent_name='hallucination_grader', model='GROQ', get_model=<function get_groq at 0x000001EBACAD77E0>, temperature=0, prompt=ChatPromptTemplate(input_variables=['documents', 'generation'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['documents'], input_types={}, partial_variables={}, template="You are a grader assessing whether an answer is grounded in / supported by a set of facts. \n \n                Give a binary score 'yes' or 'no' score to indicate\n\n                whether the answer is grounded in or supported by a set of facts. Provide the binary score with no preamble or explanation.\n\n                Here are the facts:\n{documents}\n"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['generation'], input_types={}, partial_variables={}, template='{generation}'), additional_kwargs={})]), parser=<class 'langchain_core.output_parsers.string.StrOutputParser'>), 'answer_grader': Agent(agent_name='answer_grader', model='GROQ', get_model=<function get_groq at 0x000001EBACAD77E0>, temperature=0, prompt=ChatPromptTemplate(input_variables=['generation', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template="You are a grader assessing whether an  answer is useful to resolve a question.\n\n                Give a binary score 'yes' or 'no' to indicate whether the answer is useful to resolve a question.\n\n                Provide the binary score with no preamble or explanation.\n                Here is the question:\n{question}\n"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['generation'], input_types={}, partial_variables={}, template='{generation}'), additional_kwargs={})]), parser=<class 'langchain_core.output_parsers.string.StrOutputParser'>)}
[INFO|config|L229] 2024-09-29 23:11:32: Connecting to an existing index of PineCone DB cient
[INFO|discover_namespace_packages|L12] 2024-09-29 23:11:32: Discovering subpackages in _NamespacePath(['C:\\Users\\Jorge\\Desktop\\Boe ChatBot\\backend\\env\\Lib\\site-packages\\pinecone_plugins'])
[INFO|discover_plugins|L9] 2024-09-29 23:11:32: Looking for plugins in pinecone_plugins.assistant
[INFO|discover_plugins|L9] 2024-09-29 23:11:32: Looking for plugins in pinecone_plugins.inference
[INFO|installation|L10] 2024-09-29 23:11:32: Installing plugin assistant into Pinecone
[INFO|installation|L10] 2024-09-29 23:11:32: Installing plugin inference into Pinecone
[INFO|config|L236] 2024-09-29 23:11:32: existing_indexes : ['boe']
[INFO|SentenceTransformer|L196] 2024-09-29 23:11:33: Use pytorch device_name: cpu
[INFO|SentenceTransformer|L204] 2024-09-29 23:11:33: Load pretrained SentenceTransformer: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2
[INFO|main|L67] 2024-09-29 23:11:35: config_graph : ConfigGraph(config_path='C:\\Users\\Jorge\\Desktop\\Boe ChatBot\\backend\\src\\..\\config/graph\\graph.json', graph=None, compile_graph=None)
[INFO|main|L69] 2024-09-29 23:11:35: Creating graph and compiling workflow...
[INFO|main|L81] 2024-09-29 23:11:35: Graph and workflow created
[INFO|main|L65] 2024-09-29 23:12:05: Getting Graph configuration from CONFIG_PATH='C:\\Users\\Jorge\\Desktop\\Boe ChatBot\\backend\\src\\..\\config/graph\\graph.json'
[INFO|config|L93] 2024-09-29 23:12:05: Definida configuracion mediante archivo JSON en C:\Users\Jorge\Desktop\Boe ChatBot\backend\src\..\config/graph\graph.json
[INFO|config|L224] 2024-09-29 23:12:05: Agent query_classificator -> Parser <class 'langchain_core.output_parsers.string.StrOutputParser'>
[INFO|config|L224] 2024-09-29 23:12:05: Agent docs_grader -> Parser <class 'langchain_core.output_parsers.string.StrOutputParser'>
[INFO|config|L224] 2024-09-29 23:12:05: Agent query_processor -> Parser <class 'langchain_core.output_parsers.string.StrOutputParser'>
[INFO|config|L224] 2024-09-29 23:12:05: Agent generator -> Parser <class 'langchain_core.output_parsers.string.StrOutputParser'>
[INFO|config|L224] 2024-09-29 23:12:05: Agent hallucination_grader -> Parser <class 'langchain_core.output_parsers.string.StrOutputParser'>
[INFO|config|L224] 2024-09-29 23:12:05: Agent answer_grader -> Parser <class 'langchain_core.output_parsers.string.StrOutputParser'>
[INFO|config|L150] 2024-09-29 23:12:05: Graph Agents : {'query_classificator': Agent(agent_name='query_classificator', model='GROQ', get_model=<function get_groq at 0x000001FAECD0ECA0>, temperature=0, prompt=ChatPromptTemplate(input_variables=['labels', 'text'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['labels'], input_types={}, partial_variables={}, template="You are an assistant specialized in categorizing a text from the Spanish Boletín Oficial del Estado (BOE).\n\n            Your task is to classify the provided text using the specified list of labels. The possible labels are: {labels}\n\n            If the text does not belong to a label, classify it as 'Otra'. Provide the label with no preamble or explanation"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='{text}'), additional_kwargs={})]), parser=<class 'langchain_core.output_parsers.string.StrOutputParser'>), 'docs_grader': Agent(agent_name='docs_grader', model='GROQ', get_model=<function get_groq at 0x000001FAECD0ECA0>, temperature=0, prompt=ChatPromptTemplate(input_variables=['document', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['document'], input_types={}, partial_variables={}, template="You are an AI model designed grade the relevance of a retrieved document to a user question.\n\n                If the document contains keywords related to the user question, grade it as relevant.\n\n                Give a binary score of 'yes' or 'no' to indicate whether the document is relevant to the question. \n\n                Provide the binary score with no explanation.\n\n                Here is the retrieved document: {document} "), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})]), parser=<class 'langchain_core.output_parsers.string.StrOutputParser'>), 'query_processor': Agent(agent_name='query_processor', model='GROQ', get_model=<function get_groq at 0x000001FAECD0ECA0>, temperature=0, prompt=ChatPromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a question re-writer that converts an input question to a better version that is optimized for vectorstore retrieval. \n\n                    Look at the input and try to reason about the underlying semantic intent / meaning.\n\n                    Provide the reprocessed question in spanish with no preamble or explanation'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})]), parser=<class 'langchain_core.output_parsers.string.StrOutputParser'>), 'generator': Agent(agent_name='generator', model='GROQ', get_model=<function get_groq at 0x000001FAECD0ECA0>, temperature=0, prompt=ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template="You are an assistant for question-answering tasks.\n\n                    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say 'I you don't know'.\n\n                    Use three sentences maximum and keep the answer concise.\n\n                    Context:\n{context}\n"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})]), parser=<class 'langchain_core.output_parsers.string.StrOutputParser'>), 'hallucination_grader': Agent(agent_name='hallucination_grader', model='GROQ', get_model=<function get_groq at 0x000001FAECD0ECA0>, temperature=0, prompt=ChatPromptTemplate(input_variables=['documents', 'generation'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['documents'], input_types={}, partial_variables={}, template="You are a grader assessing whether an answer is grounded in / supported by a set of facts. \n \n                Give a binary score 'yes' or 'no' score to indicate\n\n                whether the answer is grounded in or supported by a set of facts. Provide the binary score with no preamble or explanation.\n\n                Here are the facts:\n{documents}\n"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['generation'], input_types={}, partial_variables={}, template='{generation}'), additional_kwargs={})]), parser=<class 'langchain_core.output_parsers.string.StrOutputParser'>), 'answer_grader': Agent(agent_name='answer_grader', model='GROQ', get_model=<function get_groq at 0x000001FAECD0ECA0>, temperature=0, prompt=ChatPromptTemplate(input_variables=['generation', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template="You are a grader assessing whether an  answer is useful to resolve a question.\n\n                Give a binary score 'yes' or 'no' to indicate whether the answer is useful to resolve a question.\n\n                Provide the binary score with no preamble or explanation.\n                Here is the question:\n{question}\n"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['generation'], input_types={}, partial_variables={}, template='{generation}'), additional_kwargs={})]), parser=<class 'langchain_core.output_parsers.string.StrOutputParser'>)}
[INFO|config|L229] 2024-09-29 23:12:05: Connecting to an existing index of PineCone DB cient
[INFO|discover_namespace_packages|L12] 2024-09-29 23:12:05: Discovering subpackages in _NamespacePath(['C:\\Users\\Jorge\\Desktop\\Boe ChatBot\\backend\\env\\Lib\\site-packages\\pinecone_plugins'])
[INFO|discover_plugins|L9] 2024-09-29 23:12:05: Looking for plugins in pinecone_plugins.assistant
[INFO|discover_plugins|L9] 2024-09-29 23:12:05: Looking for plugins in pinecone_plugins.inference
[INFO|installation|L10] 2024-09-29 23:12:05: Installing plugin assistant into Pinecone
[INFO|installation|L10] 2024-09-29 23:12:05: Installing plugin inference into Pinecone
[INFO|config|L236] 2024-09-29 23:12:06: existing_indexes : ['boe']
[INFO|SentenceTransformer|L196] 2024-09-29 23:12:06: Use pytorch device_name: cpu
[INFO|SentenceTransformer|L204] 2024-09-29 23:12:06: Load pretrained SentenceTransformer: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2
[INFO|main|L67] 2024-09-29 23:12:08: config_graph : ConfigGraph(config_path='C:\\Users\\Jorge\\Desktop\\Boe ChatBot\\backend\\src\\..\\config/graph\\graph.json', graph=None, compile_graph=None)
[INFO|main|L69] 2024-09-29 23:12:08: Creating graph and compiling workflow...
[INFO|main|L81] 2024-09-29 23:12:08: Graph and workflow created
[INFO|main|L65] 2024-09-29 23:12:41: Getting Graph configuration from CONFIG_PATH='C:\\Users\\Jorge\\Desktop\\Boe ChatBot\\backend\\src\\..\\config/graph\\graph.json'
[INFO|config|L93] 2024-09-29 23:12:41: Definida configuracion mediante archivo JSON en C:\Users\Jorge\Desktop\Boe ChatBot\backend\src\..\config/graph\graph.json
[INFO|config|L224] 2024-09-29 23:12:41: Agent query_classificator -> Parser <class 'langchain_core.output_parsers.string.StrOutputParser'>
[INFO|config|L224] 2024-09-29 23:12:41: Agent docs_grader -> Parser <class 'langchain_core.output_parsers.string.StrOutputParser'>
[INFO|config|L224] 2024-09-29 23:12:41: Agent query_processor -> Parser <class 'langchain_core.output_parsers.string.StrOutputParser'>
[INFO|config|L224] 2024-09-29 23:12:41: Agent generator -> Parser <class 'langchain_core.output_parsers.string.StrOutputParser'>
[INFO|config|L224] 2024-09-29 23:12:41: Agent hallucination_grader -> Parser <class 'langchain_core.output_parsers.string.StrOutputParser'>
[INFO|config|L224] 2024-09-29 23:12:41: Agent answer_grader -> Parser <class 'langchain_core.output_parsers.string.StrOutputParser'>
[INFO|config|L150] 2024-09-29 23:12:41: Graph Agents : {'query_classificator': Agent(agent_name='query_classificator', model='GROQ', get_model=<function get_groq at 0x0000019786E5ED40>, temperature=0, prompt=ChatPromptTemplate(input_variables=['labels', 'text'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['labels'], input_types={}, partial_variables={}, template="You are an assistant specialized in categorizing a text from the Spanish Boletín Oficial del Estado (BOE).\n\n            Your task is to classify the provided text using the specified list of labels. The possible labels are: {labels}\n\n            If the text does not belong to a label, classify it as 'Otra'. Provide the label with no preamble or explanation"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='{text}'), additional_kwargs={})]), parser=<class 'langchain_core.output_parsers.string.StrOutputParser'>), 'docs_grader': Agent(agent_name='docs_grader', model='GROQ', get_model=<function get_groq at 0x0000019786E5ED40>, temperature=0, prompt=ChatPromptTemplate(input_variables=['document', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['document'], input_types={}, partial_variables={}, template="You are an AI model designed grade the relevance of a retrieved document to a user question.\n\n                If the document contains keywords related to the user question, grade it as relevant.\n\n                Give a binary score of 'yes' or 'no' to indicate whether the document is relevant to the question. \n\n                Provide the binary score with no explanation.\n\n                Here is the retrieved document: {document} "), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})]), parser=<class 'langchain_core.output_parsers.string.StrOutputParser'>), 'query_processor': Agent(agent_name='query_processor', model='GROQ', get_model=<function get_groq at 0x0000019786E5ED40>, temperature=0, prompt=ChatPromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a question re-writer that converts an input question to a better version that is optimized for vectorstore retrieval. \n\n                    Look at the input and try to reason about the underlying semantic intent / meaning.\n\n                    Provide the reprocessed question in spanish with no preamble or explanation'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})]), parser=<class 'langchain_core.output_parsers.string.StrOutputParser'>), 'generator': Agent(agent_name='generator', model='GROQ', get_model=<function get_groq at 0x0000019786E5ED40>, temperature=0, prompt=ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template="You are an assistant for question-answering tasks.\n\n                    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say 'I you don't know'.\n\n                    Use three sentences maximum and keep the answer concise.\n\n                    Context:\n{context}\n"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})]), parser=<class 'langchain_core.output_parsers.string.StrOutputParser'>), 'hallucination_grader': Agent(agent_name='hallucination_grader', model='GROQ', get_model=<function get_groq at 0x0000019786E5ED40>, temperature=0, prompt=ChatPromptTemplate(input_variables=['documents', 'generation'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['documents'], input_types={}, partial_variables={}, template="You are a grader assessing whether an answer is grounded in / supported by a set of facts. \n \n                Give a binary score 'yes' or 'no' score to indicate\n\n                whether the answer is grounded in or supported by a set of facts. Provide the binary score with no preamble or explanation.\n\n                Here are the facts:\n{documents}\n"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['generation'], input_types={}, partial_variables={}, template='{generation}'), additional_kwargs={})]), parser=<class 'langchain_core.output_parsers.string.StrOutputParser'>), 'answer_grader': Agent(agent_name='answer_grader', model='GROQ', get_model=<function get_groq at 0x0000019786E5ED40>, temperature=0, prompt=ChatPromptTemplate(input_variables=['generation', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template="You are a grader assessing whether an  answer is useful to resolve a question.\n\n                Give a binary score 'yes' or 'no' to indicate whether the answer is useful to resolve a question.\n\n                Provide the binary score with no preamble or explanation.\n                Here is the question:\n{question}\n"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['generation'], input_types={}, partial_variables={}, template='{generation}'), additional_kwargs={})]), parser=<class 'langchain_core.output_parsers.string.StrOutputParser'>)}
[INFO|config|L229] 2024-09-29 23:12:41: Connecting to an existing index of PineCone DB cient
[INFO|discover_namespace_packages|L12] 2024-09-29 23:12:41: Discovering subpackages in _NamespacePath(['C:\\Users\\Jorge\\Desktop\\Boe ChatBot\\backend\\env\\Lib\\site-packages\\pinecone_plugins'])
[INFO|discover_plugins|L9] 2024-09-29 23:12:41: Looking for plugins in pinecone_plugins.assistant
[INFO|discover_plugins|L9] 2024-09-29 23:12:41: Looking for plugins in pinecone_plugins.inference
[INFO|installation|L10] 2024-09-29 23:12:41: Installing plugin assistant into Pinecone
[INFO|installation|L10] 2024-09-29 23:12:41: Installing plugin inference into Pinecone
[INFO|config|L236] 2024-09-29 23:12:41: existing_indexes : ['boe']
[INFO|SentenceTransformer|L196] 2024-09-29 23:12:42: Use pytorch device_name: cpu
[INFO|SentenceTransformer|L204] 2024-09-29 23:12:42: Load pretrained SentenceTransformer: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2
[INFO|main|L67] 2024-09-29 23:12:44: config_graph : ConfigGraph(config_path='C:\\Users\\Jorge\\Desktop\\Boe ChatBot\\backend\\src\\..\\config/graph\\graph.json', graph=None, compile_graph=None)
[INFO|main|L69] 2024-09-29 23:12:44: Creating graph and compiling workflow...
[INFO|main|L81] 2024-09-29 23:12:44: Graph and workflow created
[INFO|main|L65] 2024-09-30 20:33:55: Getting Graph configuration from CONFIG_PATH='C:\\Users\\Jorge\\Desktop\\Boe ChatBot\\backend\\src\\..\\config/graph\\graph.json'
[INFO|config|L93] 2024-09-30 20:33:55: Definida configuracion mediante archivo JSON en C:\Users\Jorge\Desktop\Boe ChatBot\backend\src\..\config/graph\graph.json
[INFO|config|L224] 2024-09-30 20:33:55: Agent query_classificator -> Parser <class 'langchain_core.output_parsers.string.StrOutputParser'>
[INFO|config|L224] 2024-09-30 20:33:55: Agent docs_grader -> Parser <class 'langchain_core.output_parsers.string.StrOutputParser'>
[INFO|config|L224] 2024-09-30 20:33:55: Agent query_processor -> Parser <class 'langchain_core.output_parsers.string.StrOutputParser'>
[INFO|config|L224] 2024-09-30 20:33:55: Agent generator -> Parser <class 'langchain_core.output_parsers.string.StrOutputParser'>
[INFO|config|L224] 2024-09-30 20:33:55: Agent hallucination_grader -> Parser <class 'langchain_core.output_parsers.string.StrOutputParser'>
[INFO|config|L224] 2024-09-30 20:33:55: Agent answer_grader -> Parser <class 'langchain_core.output_parsers.string.StrOutputParser'>
[INFO|config|L150] 2024-09-30 20:33:55: Graph Agents : {'query_classificator': Agent(agent_name='query_classificator', model='GROQ', get_model=<function get_groq at 0x000001599883FD80>, temperature=0, prompt=ChatPromptTemplate(input_variables=['labels', 'text'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['labels'], input_types={}, partial_variables={}, template="You are an assistant specialized in categorizing a text from the Spanish Boletín Oficial del Estado (BOE).\n\n            Your task is to classify the provided text using the specified list of labels. The possible labels are: {labels}\n\n            If the text does not belong to a label, classify it as 'Otra'. Provide the label with no preamble or explanation"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='{text}'), additional_kwargs={})]), parser=<class 'langchain_core.output_parsers.string.StrOutputParser'>), 'docs_grader': Agent(agent_name='docs_grader', model='GROQ', get_model=<function get_groq at 0x000001599883FD80>, temperature=0, prompt=ChatPromptTemplate(input_variables=['document', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['document'], input_types={}, partial_variables={}, template="You are an AI model designed grade the relevance of a retrieved document to a user question.\n\n                If the document contains keywords related to the user question, grade it as relevant.\n\n                Give a binary score of 'yes' or 'no' to indicate whether the document is relevant to the question. \n\n                Provide the binary score with no explanation.\n\n                Here is the retrieved document: {document} "), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})]), parser=<class 'langchain_core.output_parsers.string.StrOutputParser'>), 'query_processor': Agent(agent_name='query_processor', model='GROQ', get_model=<function get_groq at 0x000001599883FD80>, temperature=0, prompt=ChatPromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a question re-writer that converts an input question to a better version that is optimized for vectorstore retrieval. \n\n                    Look at the input and try to reason about the underlying semantic intent / meaning.\n\n                    Provide the reprocessed question in spanish with no preamble or explanation'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})]), parser=<class 'langchain_core.output_parsers.string.StrOutputParser'>), 'generator': Agent(agent_name='generator', model='GROQ', get_model=<function get_groq at 0x000001599883FD80>, temperature=0, prompt=ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template="You are an assistant for question-answering tasks.\n\n                    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say 'I you don't know'.\n\n                    Use three sentences maximum and keep the answer concise.\n\n                    Context:\n{context}\n"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})]), parser=<class 'langchain_core.output_parsers.string.StrOutputParser'>), 'hallucination_grader': Agent(agent_name='hallucination_grader', model='GROQ', get_model=<function get_groq at 0x000001599883FD80>, temperature=0, prompt=ChatPromptTemplate(input_variables=['documents', 'generation'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['documents'], input_types={}, partial_variables={}, template="You are a grader assessing whether an answer is grounded in / supported by a set of facts. \n \n                Give a binary score 'yes' or 'no' score to indicate\n\n                whether the answer is grounded in or supported by a set of facts. Provide the binary score with no preamble or explanation.\n\n                Here are the facts:\n{documents}\n"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['generation'], input_types={}, partial_variables={}, template='{generation}'), additional_kwargs={})]), parser=<class 'langchain_core.output_parsers.string.StrOutputParser'>), 'answer_grader': Agent(agent_name='answer_grader', model='GROQ', get_model=<function get_groq at 0x000001599883FD80>, temperature=0, prompt=ChatPromptTemplate(input_variables=['generation', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template="You are a grader assessing whether an  answer is useful to resolve a question.\n\n                Give a binary score 'yes' or 'no' to indicate whether the answer is useful to resolve a question.\n\n                Provide the binary score with no preamble or explanation.\n                Here is the question:\n{question}\n"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['generation'], input_types={}, partial_variables={}, template='{generation}'), additional_kwargs={})]), parser=<class 'langchain_core.output_parsers.string.StrOutputParser'>)}
[INFO|config|L229] 2024-09-30 20:33:55: Connecting to an existing index of PineCone DB cient
[INFO|discover_namespace_packages|L12] 2024-09-30 20:33:55: Discovering subpackages in _NamespacePath(['C:\\Users\\Jorge\\Desktop\\Boe ChatBot\\backend\\env\\Lib\\site-packages\\pinecone_plugins'])
[INFO|discover_plugins|L9] 2024-09-30 20:33:55: Looking for plugins in pinecone_plugins.assistant
[INFO|discover_plugins|L9] 2024-09-30 20:33:56: Looking for plugins in pinecone_plugins.inference
[INFO|installation|L10] 2024-09-30 20:33:56: Installing plugin assistant into Pinecone
[INFO|installation|L10] 2024-09-30 20:33:56: Installing plugin inference into Pinecone
[INFO|config|L236] 2024-09-30 20:33:57: existing_indexes : ['boe']
[INFO|SentenceTransformer|L196] 2024-09-30 20:33:57: Use pytorch device_name: cpu
[INFO|SentenceTransformer|L204] 2024-09-30 20:33:57: Load pretrained SentenceTransformer: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2
[INFO|main|L67] 2024-09-30 20:33:59: config_graph : ConfigGraph(config_path='C:\\Users\\Jorge\\Desktop\\Boe ChatBot\\backend\\src\\..\\config/graph\\graph.json', graph=None, compile_graph=None)
[INFO|main|L69] 2024-09-30 20:33:59: Creating graph and compiling workflow...
[INFO|main|L81] 2024-09-30 20:33:59: Graph and workflow created
